---
title: "语言模型和词向量"
date: 2025-04-02T23:49:51+08:00
lastmod: 2025-04-02T23:49:51+08:00
author: ["Chang Liu"]
tags: 
- read
- money
summary: "语言模型和词向量笔记， deepseek 优化版"
weight: # 输入1可以顶置文章，用来给文章展示排序，不填就默认按时间排序
slug: ""
draft: false # 是否为草稿
comments: true
showToc: true # 显示目录
TocOpen: true # 自动展开目录
hidemeta:  true # 是否隐藏文章的元信息，如发布日期、作者等
disableShare: true # 底部不显示分享栏
showbreadcrumbs: true #顶部显示当前路径
cover:
    image: ""
    caption: ""
    alt: ""
    relative: false
---


---

### **语言模型和词向量**

今天在看一本关于 GPT 入门的书，学习了关于语言模型和词向量的部分。这里以外行小白的视角来介绍一下这两个东西。

---

#### **1. 语言模型：从规则到统计**  
语言模型的目标是让计算机“理解”人类语言，进而完成文本补全等任务。这里的“理解”并非真正的认知，而是通过概率预测下一个词的可能性。  

早期有两种思路：  
- **规则系统**：人工编写语法规则。例如，规定“名词后面可能接动词”。但中文语法复杂，规则难以穷举，效果很差。  
- **统计模型**：从数据中学习规律。例如，给定前 N 个词，统计下一个词的概率分布。这就是 **N-gram 模型** 的核心思想。  

以中文为例，若用 2-gram 模型，训练时需要将文本切分为词语序列（如鲁迅的小说会被分成“鲁迅”“的”“小说”等词），然后统计词频。例如，若语料中出现“鲁迅的小说”100次，“小说的年代”50次，则“鲁迅”后接“的”的概率为 100/(100+50) ≈ 67%。通过这种方式，模型可以模拟鲁迅的风格生成文本。

---

#### **2. 词向量革命：Word2Vec 与 CBOW**  
词向量是将词语映射到低维向量的技术，经典实现是 Word2Vec 的 **CBOW（Continuous Bag of Words）模型**。它的核心思想是：通过上下文预测目标词。  

假设我们有一个包含 7 个词的小语料库（如 ["我", "爱", "北京", "天安门", "天安", "门", "的"]），目标是训练一个 2 维的词向量。具体步骤如下：  
1. **数据准备**：将每个词转为 one-hot 编码（长度为 7）。例如，“北京”对应 `[0,0,1,0,0,0,0]`。  
2. **输入上下文**：CBOW 的输入是目标词的上下文词。例如，若窗口大小为 2，目标词“爱”的上下文是“我”和“北京”。  
3. **嵌入层**：将两个 one-hot 向量映射到 2 维空间，得到稠密向量（如 `[0.2, 0.8]` 和 `[0.6, 0.4]`）。  
4. **预测目标词**：将上下文向量平均后输入全连接层，输出概率分布（如预测“天安门”的概率最高）。  

训练完成后，嵌入矩阵的每一行对应一个词的向量。虽然高维空间中的语义关系无法直接解释，但相似词（如“猫”和“狗”）往往距离较近。例如，在理想情况下，可能观察到类似 `国王 - 男人 + 女人 ≈ 王后` 的语义运算，但这高度依赖训练数据和模型规模。

---

#### **3. 争议与局限**  
- **数据依赖性**：词向量的质量严重依赖语料库。若数据量不足，模型可能学到错误关联（如“医生 - 男性 + 女性 = 护士”）。  
- **语义黑箱**：低维向量中的“语义方向”是数学推导的结果，而非明确的人类定义属性。  

尽管如此，词向量仍是自然语言处理的重要基石。后续的 Transformer、BERT 等模型，在此基础上进一步提升了语言理解的深度。  

--- 

以上是我的学习笔记，欢迎指正！


----

原文

```

今天在看一本关于 GPT 入门的书，学习了关于语言模型和词向量的部分。这儿以外行小白的视角来介绍一下这两个东西。

首先是语言模型，也就是怎么对人类语言模型建模，让计算机可以理解人类的语言。这儿的“理解”不一定是要真的理解，而是计算机可以完成文本补全之类的任务。第一个选择就是规则系统，把人类语言的语法作为规则编码进去。可想而知，这个很难做，效果也不好。第二个选择则是从概率统计的角度来看，计算机只要知道：给定 N 个单词，知道下一个单词可以为哪些单词，它们各自的概率是多少。

所以就有了 N-GRAM 模型，用前 N 个单词作为前缀来判定下一个单词的概率。这个比较容易理解，训练过程也很直观了。你可以拿一个语料，比如鲁迅的小说，把他们按字切分后，每 N+1 个字生成一个 <C1, C2, … Cn, Cn+1> 元素，然后统计他们出现的次数。之后就可以模拟鲁迅生成文字。

关于词向量，当年 word2vec 特别火。它能实现词汇的加减法。比如说： `国王 - 男人 + 女人 = 王后`。 它的原理是什么呢？

以其中的 CBOW 算法为例子，假设我们只有  7 个单词的文本库， 训练一个 2 维的词向量。 我们首先把文本库生成训练数据，对于每 N 个临近词生成：<W1, W2> 的元素，再用 one-shot 和单词下标来替换 W1 W2 得到类似 <<0, 0, 0, 1, 0, 0, 0>, 3> 的数据。 这儿 W1 下标为 4， W2 下标为 3。然后用一个简单的神经网络，这个网络的一层参数会是 7*2 的矩阵，输出一个 softmax 的 1*7 的矩阵。 这儿 <0, 0, 0, 1, 0, 0, 0> 作为网络输入， 3 作为答案来训练网络，最后就能得到 7*2 的权重矩阵。这个矩阵也就是这 7 个单词的词向量。 把它们画到图上，就会发现相关的词离得近，不相关的词离得远。

本质上应该是：把文本人为切分生成了训练集，他们之间可能有种数据关系，通过神经网络的训练找到在指定维度向量上的表示。如果数据量够大，维度够多，每一维可能会对应到一个属性：比如是不是颜色，是不是生活物品。

这个部分感觉还是挺简单优美且神奇的。我这儿介绍的比较外行，大家有兴趣可以查阅相关的文章。

```
